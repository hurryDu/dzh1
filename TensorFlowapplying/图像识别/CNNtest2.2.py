import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models, regularizers
from tensorflow.keras.datasets import cifar10
import tensorflow.keras.backend as K
from PIL import Image
import os
import pickle
import random
from sklearn.model_selection import train_test_split

# é™åˆ¶å†…å­˜ä½¿ç”¨
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
else:
    # æ›´ä¸¥æ ¼çš„CPUå†…å­˜é™åˆ¶
    tf.config.threading.set_intra_op_parallelism_threads(1)
    tf.config.threading.set_inter_op_parallelism_threads(1)
    # é™åˆ¶TensorFlowå†…å­˜å¢é•¿
    tf.config.set_soft_device_placement(True)


# åœ¨æ¯æ¬¡è®­ç»ƒåæ¸…ç†å†…å­˜
def clear_memory():
    K.clear_session()
    import gc
    gc.collect()


# è®¾ç½®éšæœºç§å­
SEED = 42
np.random.seed(SEED)
tf.random.set_seed(SEED)
random.seed(SEED)

# å®šä¹‰ç±»åˆ«åç§°
class_names = [
    "airplane",  # 0
    "automobile",  # 1
    "bird",  # 2
    "frog",  # 3
    "dog",  # 4
    "cat",  # 5
    "horse",  # 6
    "ship",  # 7
    "truck",  # 8
    "deer"  # 9
]


# æ„å»ºè¶…è½»é‡çº§é«˜æ•ˆæ¨¡å‹
def build_lightweight_model():
    model = models.Sequential([
        # æ•°æ®å¢å¼ºå±‚ (ä¸å¢åŠ å†…å­˜è´Ÿæ‹…çš„ç±»å‹)
        layers.experimental.preprocessing.RandomFlip("horizontal"),

        # ç¬¬ä¸€ä¸ªå·ç§¯å—
        layers.Conv2D(16, 3, padding='same', activation='relu',
                      kernel_regularizer=regularizers.l2(1e-4),
                      input_shape=(32, 32, 3)),
        layers.BatchNormalization(),
        layers.MaxPooling2D(2),

        # ç¬¬äºŒä¸ªå·ç§¯å—
        layers.Conv2D(32, 3, padding='same', activation='relu',
                      kernel_regularizer=regularizers.l2(1e-4)),
        layers.BatchNormalization(),
        layers.MaxPooling2D(2),
        layers.Dropout(0.25),

        # ç¬¬ä¸‰ä¸ªå·ç§¯å—
        layers.Conv2D(64, 3, padding='same', activation='relu',
                      kernel_regularizer=regularizers.l2(1e-4)),
        layers.BatchNormalization(),
        layers.MaxPooling2D(2),
        layers.Dropout(0.25),

        # åˆ†ç±»å™¨å¤´éƒ¨
        layers.Flatten(),
        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),
        layers.BatchNormalization(),
        layers.Dropout(0.5),
        layers.Dense(10, activation='softmax')
    ])

    # ç¼–è¯‘æ¨¡å‹
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    return model


# æ•°æ®å¤„ç†å‡½æ•°
def load_and_preprocess_data():
    # åŠ è½½æ•°æ®
    (X_train, y_train), (X_test, y_test) = cifar10.load_data()

    # æ ‡å‡†åŒ– - ä½¿ç”¨ç®€å•çš„å½’ä¸€åŒ–ä»¥å‡å°‘è®¡ç®—
    X_train = X_train.astype('float32') / 255.0
    X_test = X_test.astype('float32') / 255.0

    # è½¬æ¢æ ‡ç­¾
    y_train = tf.keras.utils.to_categorical(y_train, 10)
    y_test = tf.keras.utils.to_categorical(y_test, 10)

    # åˆ›å»ºéªŒè¯é›†
    X_train, X_val, y_train, y_val = train_test_split(
        X_train, y_train, test_size=0.1, random_state=SEED
    )

    return (X_train, y_train), (X_val, y_val), (X_test, y_test)


# è‡ªå®šä¹‰æ•°æ®ç”Ÿæˆå™¨ - å†…å­˜å‹å¥½å‹
def data_generator(X, y, batch_size=32):
    num_samples = X.shape[0]
    while True:
        indices = np.random.permutation(num_samples)
        for i in range(0, num_samples, batch_size):
            batch_indices = indices[i:i + batch_size]
            X_batch = X[batch_indices]
            y_batch = y[batch_indices]

            # ç®€å•å¢å¼º
            for j in range(len(X_batch)):
                if np.random.random() > 0.5:
                    X_batch[j] = np.fliplr(X_batch[j])

            yield X_batch, y_batch


# å†å²è®°å½•ç›¸å…³å‡½æ•°
def save_history(history, filename='lightweight_history.pkl'):
    with open(filename, 'wb') as file:
        pickle.dump(history, file)
    print(f"Training history saved to {filename}")


def load_history(filename='lightweight_history.pkl'):
    if os.path.exists(filename):
        with open(filename, 'rb') as file:
            return pickle.load(file)
    return {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}


def merge_history(old_history, new_history):
    merged = {}
    for key in new_history:
        if key in old_history:
            merged[key] = old_history[key] + new_history[key]
        else:
            merged[key] = new_history[key]
    return merged


# æ˜¾ç¤ºè®­ç»ƒå†å²
def show_training_history(history):
    plt.figure(figsize=(12, 4))

    # å‡†ç¡®ç‡å›¾è¡¨
    plt.subplot(1, 2, 1)
    plt.plot(history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.get('val_accuracy', []), label='Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.ylim([0, 1])
    plt.legend(loc='lower right')
    plt.title('Model Accuracy')

    # æŸå¤±å›¾è¡¨
    plt.subplot(1, 2, 2)
    plt.plot(history.get('loss', []), label='Training Loss')
    plt.plot(history.get('val_loss', []), label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend(loc='upper right')
    plt.title('Model Loss')

    plt.tight_layout()
    plt.show()


# ç»§ç»­è®­ç»ƒæ¨¡å‹
def continue_training_model(epochs=5, batch_size=16):
    model_path = 'lightweight_cifar10.h5'
    best_model_path = 'best_' + model_path

    # åŠ è½½ä¿å­˜çš„æ¨¡å‹
    if os.path.exists(model_path):
        model = tf.keras.models.load_model(model_path)
        print("Loaded existing model for continued training.")
    else:
        model = build_lightweight_model()
        print("Building new model for training.")

    # åŠ è½½æ•°æ®
    (X_train, y_train), (X_val, y_val), (X_test, y_test) = load_and_preprocess_data()

    # åˆ›å»ºæ•°æ®ç”Ÿæˆå™¨
    train_gen = data_generator(X_train, y_train, batch_size)
    steps_per_epoch = len(X_train) // batch_size

    # å®šä¹‰å›è°ƒ
    callbacks = [
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=1e-6,
            verbose=1
        ),
        tf.keras.callbacks.ModelCheckpoint(
            best_model_path,
            save_best_only=True,
            monitor='val_loss',
            mode='min',
            verbose=1
        ),
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True,
            verbose=1
        )
    ]

    # è®­ç»ƒæ¨¡å‹
    print(f"Training for {epochs} epochs with batch size {batch_size}...")
    history = model.fit(
        train_gen,
        steps_per_epoch=steps_per_epoch,
        epochs=epochs,
        validation_data=(X_val, y_val),
        callbacks=callbacks,
        verbose=1
    )

    # ä¿å­˜æ¨¡å‹å’Œå†å²è®°å½•
    model.save(model_path)

    # åˆå¹¶å†å²è®°å½•
    old_history = load_history()
    merged_history = merge_history(old_history, history.history)
    save_history(merged_history)

    # è¯„ä¼°æ¨¡å‹
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)
    print(f"Test accuracy: {test_acc:.4f}")
    print(f"Test loss: {test_loss:.4f}")

    # æ¸…ç†å†…å­˜
    clear_memory()

    return model


# å®ç°çŸ¥è¯†è’¸é¦è®­ç»ƒï¼ˆä»å¤æ‚æ¨¡å‹åˆ°ç®€å•æ¨¡å‹çš„çŸ¥è¯†è½¬ç§»ï¼‰
def distillation_training(epochs=5, batch_size=16, alpha=0.1, temperature=5):
    model_path = 'lightweight_cifar10.h5'

    # åŠ è½½ä¿å­˜çš„æ¨¡å‹
    if os.path.exists(model_path):
        model = tf.keras.models.load_model(model_path)
        print("Loaded existing model for distillation training.")
    else:
        model = build_lightweight_model()
        print("Building new model for distillation training.")

    # åŠ è½½æ•°æ®
    (X_train, y_train), (X_val, y_val), (X_test, y_test) = load_and_preprocess_data()

    # å®šä¹‰çŸ¥è¯†è’¸é¦æŸå¤±å‡½æ•°
    def distillation_loss(y_true, y_pred):
        # çœŸå®æ ‡ç­¾çš„ç¡¬æŸå¤±
        y_true_hard = tf.cast(y_true, tf.float32)
        hard_loss = tf.keras.losses.categorical_crossentropy(y_true_hard, y_pred)

        # ä½¿ç”¨è‡ªå·±çš„é¢„æµ‹ä½œä¸ºè½¯æ ‡ç­¾ - è‡ªè’¸é¦
        # è¿™é€šè¿‡è¿­ä»£æ”¹è¿›ä½¿æ¨¡å‹ä¸å…¶æ›´ç¨³å®šçš„ç‰ˆæœ¬ä¸€è‡´
        y_soft = tf.nn.softmax(y_pred / temperature)
        soft_targets = tf.stop_gradient(y_soft)  # ä¸é€šè¿‡è½¯ç›®æ ‡ä¼ æ’­æ¢¯åº¦
        soft_loss = tf.keras.losses.categorical_crossentropy(soft_targets, y_soft)

        return (1 - alpha) * hard_loss + alpha * soft_loss * (temperature ** 2)

    # é‡æ–°ç¼–è¯‘æ¨¡å‹ä½¿ç”¨è’¸é¦æŸå¤±
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),  # è¾ƒä½çš„å­¦ä¹ ç‡
        loss=distillation_loss,
        metrics=['accuracy']
    )

    # å®šä¹‰å›è°ƒ
    callbacks = [
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.2,
            patience=3,
            min_lr=1e-6,
            verbose=1
        ),
        tf.keras.callbacks.ModelCheckpoint(
            'distilled_' + model_path,
            save_best_only=True,
            monitor='val_loss',
            mode='min',
            verbose=1
        )
    ]

    # è®­ç»ƒæ¨¡å‹
    print(f"Training with knowledge distillation for {epochs} epochs...")
    history = model.fit(
        X_train, y_train,
        batch_size=batch_size,
        epochs=epochs,
        validation_data=(X_val, y_val),
        callbacks=callbacks,
        verbose=1
    )

    # ä¿å­˜æ¨¡å‹å’Œå†å²è®°å½•
    model.save('distilled_' + model_path)

    # åˆå¹¶å†å²è®°å½•
    old_history = load_history()
    merged_history = merge_history(old_history, history.history)
    save_history(merged_history)

    # è¯„ä¼°æ¨¡å‹
    model.compile(  # ä½¿ç”¨æ ‡å‡†æŸå¤±è¿›è¡Œè¯„ä¼°
        optimizer=tf.keras.optimizers.Adam(),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)
    print(f"Test accuracy after distillation: {test_acc:.4f}")
    print(f"Test loss after distillation: {test_loss:.4f}")

    # æ¸…ç†å†…å­˜
    clear_memory()

    return model


# é¢„æµ‹å›¾ç‰‡å‡½æ•°
def predict_image(model, image_path):
    try:
        # éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(image_path):
            print(f"Error: File '{image_path}' does not exist.")
            return

        # åŠ è½½å›¾åƒ
        img = Image.open(image_path)

        # æ˜¾ç¤ºåŸå§‹å›¾ç‰‡
        plt.figure(figsize=(6, 6))
        plt.imshow(img)
        plt.title("Original Image")
        plt.axis('off')
        plt.show()

        # è°ƒæ•´å¤§å°ä¸º32x32åƒç´ 
        img_resized = img.resize((32, 32))

        # å°†å›¾ç‰‡è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶è¿›è¡Œé¢„å¤„ç†
        img_array = np.array(img_resized)

        # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦ä¸ºç°åº¦å›¾åƒ
        if len(img_array.shape) == 2:
            # å°†ç°åº¦å›¾åƒè½¬æ¢ä¸ºRGB
            img_array = np.stack((img_array,) * 3, axis=-1)
        elif img_array.shape[2] == 4:
            # å¤„ç†å¸¦é€æ˜é€šé“çš„å›¾åƒ
            img_array = img_array[:, :, :3]

        # å½’ä¸€åŒ–åƒç´ å€¼
        img_array = img_array.astype('float32') / 255.0

        # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        img_array = np.expand_dims(img_array, axis=0)

        # è¿›è¡Œé¢„æµ‹
        prediction = model.predict(img_array)
        predicted_class = np.argmax(prediction[0])
        confidence = prediction[0][predicted_class] * 100

        # æ˜¾ç¤ºå¤„ç†åçš„å›¾ç‰‡å’Œé¢„æµ‹ç»“æœ
        plt.figure(figsize=(6, 6))
        plt.imshow(img_resized)
        plt.title(f"Prediction: {class_names[predicted_class]}\nConfidence: {confidence:.2f}%")
        plt.axis('off')
        plt.show()

        print(f"Predicted class: {class_names[predicted_class]}")
        print(f"Confidence: {confidence:.2f}%")

        # æ˜¾ç¤ºæ‰€æœ‰ç±»åˆ«çš„ç½®ä¿¡åº¦
        plt.figure(figsize=(10, 5))
        plt.bar(class_names, prediction[0] * 100)
        plt.xlabel('Classes')
        plt.ylabel('Confidence (%)')
        plt.title('Prediction Confidence for Each Class')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

    except Exception as e:
        print(f"Error processing image: {str(e)}")
        print("Please make sure the file is a valid image and you have permission to access it.")
        import traceback
        traceback.print_exc()


# ä¸»å‡½æ•°
def main():
    model_path = 'lightweight_cifar10.h5'

    # æ£€æŸ¥æ˜¯å¦æœ‰ä¿å­˜çš„æ¨¡å‹
    if os.path.exists(model_path):
        print("Loading existing model...")
        model = tf.keras.models.load_model(model_path)
    else:
        # å¦‚æœæ²¡æœ‰ä¿å­˜çš„æ¨¡å‹ï¼Œè®­ç»ƒä¸€ä¸ªæ–°æ¨¡å‹
        print("No existing model found. Starting initial training...")
        model = build_lightweight_model()

        # åŠ è½½æ•°æ®
        (X_train, y_train), (X_val, y_val), _ = load_and_preprocess_data()

        # åˆå§‹è®­ç»ƒ
        print("Starting initial training with 5 epochs...")
        callback = tf.keras.callbacks.ModelCheckpoint(
            'best_' + model_path,
            save_best_only=True,
            monitor='val_loss',
            mode='min'
        )

        history = model.fit(
            X_train, y_train,
            batch_size=16,  # å°æ‰¹é‡å‡å°‘å†…å­˜éœ€æ±‚
            epochs=5,  # å¼€å§‹æ—¶åªè®­ç»ƒå¾ˆå°‘çš„è½®æ¬¡
            validation_data=(X_val, y_val),
            callbacks=[callback],
            verbose=1
        )

        # ä¿å­˜åˆå§‹å†å²å’Œæ¨¡å‹
        save_history(history.history)
        model.save(model_path)
        print("Initial model saved.")

        # æ¸…ç†å†…å­˜
        clear_memory()

    # å¾ªç¯äº¤äº’ç•Œé¢
    while True:
        # æ˜¾ç¤ºå½“å‰æ¨¡å‹æ€§èƒ½
        if os.path.exists('lightweight_history.pkl'):
            history = load_history()
            current_epoch = len(history.get('loss', []))
            last_val_acc = history.get('val_accuracy', [0])[-1] if history.get('val_accuracy') else 0
            last_val_loss = history.get('val_loss', [0])[-1] if history.get('val_loss') else 0

            print(f"\nCurrent model status:")
            print(f"Total epochs trained: {current_epoch}")
            print(f"Last validation accuracy: {last_val_acc:.4f}")
            print(f"Last validation loss: {last_val_loss:.4f}")

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
            if last_val_loss < 0.1:
                print("\nğŸ‰ ç›®æ ‡è¾¾æˆ! éªŒè¯æŸå¤±å·²ä½äº0.1")

        # è¯¢é—®ç”¨æˆ·è¾“å…¥
        print("\nOptions:")
        print("1. Classify a new image")
        print("2. Continue standard training (5-10 epochs)")
        print("3. Run knowledge distillation (improve low loss)")
        print("4. View training history")
        print("5. Exit")
        choice = input("Enter your choice (1-5): ")

        if choice == '1':
            print("\nPlease enter the complete path to your image file:")
            print("Example: C:\\Users\\Username\\Pictures\\cat.jpg")
            image_path = input("Enter the path to your image file: ")
            predict_image(model, image_path)
        elif choice == '2':
            try:
                epochs = int(input("Enter number of epochs (5-10 recommended): "))
                batch_size = int(input("Enter batch size (8-32 recommended): "))
                if epochs < 1 or batch_size < 1:
                    print("Values must be at least 1.")
                    continue
                model = continue_training_model(epochs, batch_size)
            except ValueError:
                print("Please enter valid numbers.")
        elif choice == '3':
            # çŸ¥è¯†è’¸é¦è®­ç»ƒ - ç‰¹åˆ«æœ‰åŠ©äºè·å¾—éå¸¸ä½çš„æŸå¤±
            try:
                epochs = int(input("Enter number of epochs for distillation (3-5 recommended): "))
                if epochs < 1:
                    print("Number of epochs must be at least 1.")
                    continue
                model = distillation_training(epochs=epochs, batch_size=16)
            except ValueError:
                print("Please enter valid numbers.")
        elif choice == '4':
            if os.path.exists('lightweight_history.pkl'):
                history = load_history()
                show_training_history(history)
            else:
                print("No training history found.")
        elif choice == '5':
            print("Exiting program.")
            break
        else:
            print("Invalid choice. Please try again.")


# å¯åŠ¨ç¨‹åº
if __name__ == "__main__":
    main()